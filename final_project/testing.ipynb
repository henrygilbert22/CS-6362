{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Source: https://github.com/nnormandin/Conditional_VAE/blob/master/Conditional_VAE.ipynb\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import concatenate as concat\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import\n",
    "The really convenient load_data method pulls in MNIST data that is already separated into training and test partitions, with separate X (pixel representation) and y (label value). The X matrices are 28x28 numpy arrays, while the y is just an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping\n",
    "We have to do two things here:\n",
    "\n",
    "1: Properly represent the pixel information contained in X to a fully-connected feed forward neural network\n",
    "\n",
    "I'm planning on using a fully-connected (dense) layer to look at the MNIST pixel information because 784 pixels (28x28) isn't really that big. We could alternatively do some convolution and retain the information involved in the spatial distribution of the pixels, but in this notebook we'll stick with fully-connected layers. First we'll convert the matrices to 32-bit floating point values and normalize. Then we'll reshape the matrices to make them flat vector respresentations of the 784 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "n_pixels = np.prod(X_train.shape[1:])\n",
    "X_train = X_train.reshape((len(X_train), n_pixels))\n",
    "X_test = X_test.reshape((len(X_test), n_pixels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Properly represent the label y\n",
    "\n",
    "Encoding the class labels of y as the integers they represent makes intuitive sense, but the common loss functions for classification ine keras use cross-entropy and expect one-hot encoded vectors (of dimension K-1, where K is your number of classes) for class labels rather than just a 1d vector of class names. Luckily, Keras has a built-in utility function to one-hot encode classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters\n",
    "Assign the type of optimizer, batch size, latent-space represeentation size, and number of epochs. We'll also save the widths of the X and Y matrices for convenience in referencing them further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 250 # batch size\n",
    "n_z = 2 # latent space size\n",
    "encoder_dim1 = 512 # dim of encoder hidden layer\n",
    "decoder_dim = 512 # dim of decoder hidden layer\n",
    "decoder_out_dim = 784 # dim of decoder output layer\n",
    "activ = 'relu'\n",
    "optim = Adam(lr=0.001)\n",
    "\n",
    "\n",
    "n_x = X_train.shape[1]\n",
    "n_y = y_train.shape[1]\n",
    "\n",
    "\n",
    "n_epoch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder\n",
    "We'll be using the Keras functional API rather than the sequential because of the slightly more complex structure of the VAE. First we'll explicitly define input layers for X and y. Keras needs to know their shapes at the input layer, but can infer them later on.\n",
    "\n",
    "Next we'll concatenate the X and y vectors. It may appear that it would've been simpler to merge the pixel and class label vectors from the beginning (now that they're both 1d) rather than reading them into the graph as separate input layers and concatenating them... but in reality, we need them to remain separate entities so that we can properly calculate our reconstruction error (we aren't asking the autoencoder to reassemble y in addition to X).\n",
    "\n",
    "Once we've defined our inputs and merged them within the context of the graph, we'll pass them to a dense layer consisting of the previuosly specified number of neurons (512) and activation function (ReLU). That layer is then connected to layers that produce our mean () and standard deviation () for the variational sampling that occurs later.\n",
    "\n",
    "Next we define a function that adds random normal noise to our sampling process, and call it with a Lamda layer. This is really the guts of the variational part of this type of method, and you should refer to the blog post mentioned above for a good understanding of why this is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Input(shape=(n_x,))\n",
    "label = Input(shape=(n_y,))\n",
    "inputs = concat([X, label])\n",
    "\n",
    "\n",
    "encoder_h = Dense(encoder_dim1, activation=activ)(inputs)\n",
    "mu = Dense(n_z, activation='linear')(encoder_h)\n",
    "l_sigma = Dense(n_z, activation='linear')(encoder_h)\n",
    "\n",
    "def sample_z(args):\n",
    "    mu, l_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(l_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sampling latent space\n",
    "z = Lambda(sample_z, output_shape = (n_z, ))([mu, l_sigma])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent space\n",
    "Now that we've built our encoder and defined our sampling function, our latent space (z) is easy to define.\n",
    "\n",
    "First, using our sample_z function, we generate a vector of length n_z (in this case 2). If this were a normal VAE we could stop here and move on to the decoder, but instead we are going to concatenate our latent z respresentation with the same sparse y vector that we initially merged to our pixel representation X in the input layers. This gives us a 1x12 vector with 3 non-zero values as we move from the latent space to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Lambda(sample_z, output_shape = (n_z, ))([mu, l_sigma])\n",
    "\n",
    "# merge latent space with label\n",
    "zc = concat([z, label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder\n",
    "The encoder has hopefully taken the information contained in 784 pixels (plus the class label), and created some vector z. The decoding process is the reconstruction from z to X_hat. Unlike a normal undercomplete autoencoder, we won't stick to a rigid symmetrical funnel-type architecture here. Instead I'll define two dense layers of 512 and 784 neurons that have ReLU and sigmoidal activation functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = Dense(decoder_dim, activation=activ)\n",
    "decoder_out = Dense(decoder_out_dim, activation='sigmoid')\n",
    "h_p = decoder_hidden(zc)\n",
    "outputs = decoder_out(h_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss\n",
    "If you're familiar with autoencoders, you probably understand that they are backpropagated using reconstruction loss. This is a measure of error between the input X and the decoded output X_hat. In VAEs, our loss is the sum of reconstruction error and the kullback-leibler divergence between our  and log- and the standard normal.\n",
    "\n",
    "In this notebook I've defined the vae_loss function, which we'll use to optimize our model. I've also broken it down into the KL_loss and recon_loss subcomponents so that we can track these values as metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    recon = K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    kl = 0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=-1)\n",
    "    return recon + kl\n",
    "\n",
    "def KL_loss(y_true, y_pred):\n",
    "\treturn(0.5 * K.sum(K.exp(l_sigma) + K.square(mu) - 1. - l_sigma, axis=1))\n",
    "\n",
    "def recon_loss(y_true, y_pred):\n",
    "\treturn K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the graphs\n",
    "First, we can create networks of from the Keras Model class by defining the inputs and outputs of our conditional variational autoencoder, as well as the encoder/decoder subcomponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = Model([X, label], outputs)\n",
    "encoder = Model([X, label], mu)\n",
    "\n",
    "d_in = Input(shape=(n_z+n_y,))\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(d_in, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "We train the model as a whole, using the compile and fit methods for the cvae object. We'll use the optimizer that we defined previously, our custom vae_loss, and we'll also pass the KL_loss and recon_loss to the metrics argument so that they'll be tracked by batch.\n",
    "\n",
    "For the fit method we pass a list of inputs, validation data, the number of epochs, and a callback that stops the model early if validation loss hasn't improved in the past 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.compile(optimizer=optim, loss=vae_loss, metrics = [KL_loss, recon_loss])\n",
    "# compile and fit\n",
    "cvae_hist = cvae.fit([X_train, y_train], X_train, verbose = 1, batch_size=m, epochs=n_epoch,\n",
    "\t\t\t\t\t\t\tvalidation_data = ([X_test, y_test], X_test),\n",
    "\t\t\t\t\t\t\tcallbacks = [EarlyStopping(patience = 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the model\n",
    "The latent space should hopefully contain some interesting structural information about the digits we're autoencoding. That's the case in any autoencoding network, but in a VAE the spatial arrangement should make more intuitive 'sense' since the noise added to the latent space representation forces the model to create useful respresentations.\n",
    "\n",
    "Generating a latent space representation with the encoder\n",
    "First let's see concretely what happens when we pass an image and class to the encoder. We can take a look at the first image in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(28, 28), cmap = plt.cm.gray), axis('off')\n",
    "plt.show()\n",
    "print(Y_train[0])\n",
    "\n",
    "encoded_X0 = encoder.predict([X_train[0].reshape((1, 784)), y_train[0].reshape((1, 10))])\n",
    "print(encoded_X0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder maps the input data from  to . In a normal VAE, we'd expect this z vector to contain all of the information we need to reconstruct the digit. Therefore, these two numbers would contain information about which digit it is and what the style of the digit is. We'd expect to see a high degree of separation between the digits if we were to plot them. In a conditional VAE, however, we expect something different. Since we append the class label directly to the latent space representation, our network doesn't need to store any information about which digit it generates in the latent space. Instead, it can use the latent space to learn other interesting featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = encoder.predict([X_train, y_train])\n",
    "encodings= np.asarray(z_train)\n",
    "encodings = encodings.reshape(X_train.shape[0], n_z)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(encodings[:, 0], encodings[:, 1], c=Y_train, cmap=plt.cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like all of the digits (represented by the different colors) are pretty much layered on top of each other and are distributed approximately bivariate normal. This is what we would expect to happen.\n",
    "\n",
    "Generating a digit\n",
    "We've now passed images and labels to the encoder and examined the latent space representation. It's clear that the z values don't contain useful information about which digit is produced... but then what information do they contain?\n",
    "\n",
    "First, let's just generate a digit. We need to pass a vector to our decoder containing everything it needs to create a digit from the latent space. The z values are distributed normally with mean 0, so the 'default' setting of them is to be 0. We'll append the label to the z values, which is just a one-hot encoding to specify which digit we want to create. So if we want to generate a default 3, we'd pass the encoder something like [0,0,0,0,0,1,0,0,0,0,0,0]. I'll define a function to make this easier, and then display the outcome for the digit 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_numvec(digit, z = None):\n",
    "    out = np.zeros((1, n_z + n_y))\n",
    "    out[:, digit + n_z] = 1.\n",
    "    if z is None:\n",
    "        return(out)\n",
    "    else:\n",
    "        for i in range(len(z)):\n",
    "            out[:,i] = z[i]\n",
    "        return(out)\n",
    "    \n",
    "sample_3 = construct_numvec(3)\n",
    "print(sample_3)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(decoder.predict(sample_3).reshape(28,28), cmap = plt.cm.gray), axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the latent space variables\n",
    "If the label appended dictates which digit will be produced, what does the z vector actually do? We know that each z-value is approximately unit-normal. I can choose a digit and plot it as I vary z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig = 3\n",
    "sides = 8\n",
    "max_z = 1.5\n",
    "\n",
    "img_it = 0\n",
    "for i in range(0, sides):\n",
    "    z1 = (((i / (sides-1)) * max_z)*2) - max_z\n",
    "    for j in range(0, sides):\n",
    "        z2 = (((j / (sides-1)) * max_z)*2) - max_z\n",
    "        z_ = [z1, z2]\n",
    "        vec = construct_numvec(dig, z_)\n",
    "        decoded = decoder.predict(vec)\n",
    "        subplot(sides, sides, 1 + img_it)\n",
    "        img_it +=1\n",
    "        plt.imshow(decoded.reshape(28, 28), cmap = plt.cm.gray), axis('off')\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I change z1 (on the y-axis), the digit style becomes narrower. Varying the value of z2 (on the x-axis) appears to rotate the digit slightly and elongate the lower portion in relation to the upper portion. There appears to be some interaction between the two values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs-6362')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7dd7eb3fce35fa0f50760c8f8b3d129dbc0da5e6df1057aa9ef5bcae08959f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
